import pandas as pd

SAMPLES = pd.read_csv("../list_of_samples.txt", header=None, squeeze=True).tolist()

CHROMOSOMES = ['2L', '2R', '3L', '3R', 'X', '4']
# CHROMOSOMES = ['4']

def get_intervals_from_bed(bed_file):
    intervals = []
    with open(bed_file) as f:
        for line in f:
            chrom, start, end = line.strip().split()
            intervals.append(f"{chrom}:{start}-{end}")
    return intervals

INTERVALS = get_intervals_from_bed("/Genomics/ayroleslab2/scott/git/chromium/sequence_processing/snakemake/new_chroms_intervals.bed")


FASTQ_DIR = "/Genomics/ayroleslab2/scott/git/chromium/data/raw"
REF_DIR = "/Genomics/ayroleslab2/scott/git/chromium/data/ref/dmel-all-chromosome-r6.49.fasta"
PICARD_DIR = "/Genomics/ayroleslab2/scott/git/chromium/tools/picard.jar"
SAMBAMBA_DIR = "sambamba"
GATK_DIR = "/Genomics/ayroleslab2/scott/git/chromium/tools/gatk"
VARIANTS_DIR = "/Genomics/ayroleslab2/scott/git/chromium/data/ref/dbSNP_Nex_Sep28.19.vcf"
TMP_DIR = "/Genomics/ayroleslab2/scott/git/chromium/data/snakemake_tmp"
RESULTS_DIR = "/Genomics/ayroleslab2/scott/git/chromium/data/snakemake_results"

rule all:
    input:
        expand(TMP_DIR + "/count_info/alternate_counts_{chrom}.txt", chrom=CHROMOSOMES),
        expand(TMP_DIR + "/count_info/reference_counts_{chrom}.txt", chrom=CHROMOSOMES)

rule trim:
    conda:
        "crvi_env.yml"
    input:
        r1 = FASTQ_DIR + "/{sample}-read-1.fastq.gz",
        r2 = FASTQ_DIR + "/{sample}-read-4.fastq.gz"
    output:
        r1_trim = TMP_DIR + "/{sample}.trim.R1.fastq.gz",
        r2_trim = TMP_DIR + "/{sample}.trim.R2.fastq.gz"
    shell:
        """
        cutadapt -e 0.1 --overlap 2 -a AGATCGGAAGAG -A AGATCGGAAGAG \
        --minimum-length=20 --trim-n -j 0 -o {output.r1_trim} \
        -p {output.r2_trim} {input.r1} {input.r2}
        """

# The environment has a bad version of samtools, so we just load the module
rule map:
    conda:
        "crvi_env.yml"
    input:
        r1_trim = TMP_DIR + "/{sample}.trim.R1.fastq.gz",
        r2_trim = TMP_DIR + "/{sample}.trim.R2.fastq.gz"
    output:
        sort_uniq_bam = TMP_DIR + "/{sample}_sort_uniq.bam",
        sort_uniq_bam_index = TMP_DIR + "/{sample}_sort_uniq.bam.bai"
    params:
        ref_dir = REF_DIR,
        threads = 1
    shell:
        r"""
        module load samtools && \
        bwa mem -M -t {params.threads} {params.ref_dir} {input.r1_trim} {input.r2_trim} | samtools view -Sbq 1 | samtools sort -@ {params.threads} -m 3G > {output.sort_uniq_bam} && \
        samtools index {output.sort_uniq_bam}
        """


rule dedup:
    conda:
        "crvi_env.yml"
    input:
        sort_uniq_bam = TMP_DIR + "/{sample}_sort_uniq.bam"
    output:
        dedup_bam = TMP_DIR + "/{sample}_sort_uniq_dedup.bam"
    params:
        picard_dir = PICARD_DIR,
        sambamba_dir = SAMBAMBA_DIR,
        tmp_dir = TMP_DIR,
        threads = 1
    shell:
        """
        java -Xmx24g -jar {params.picard_dir} MarkDuplicates \
        I={input.sort_uniq_bam} O={output.dedup_bam} M={params.tmp_dir}/{wildcards.sample}_sort_uniq_dedup.txt && \
        {params.sambamba_dir} index -t {params.threads} {output.dedup_bam}
        """

rule add_read_groups:
    conda:
        "crvi_env.yml"
    input:
        dedup_bam = TMP_DIR + "/{sample}_sort_uniq_dedup.bam"
    output:
        dedup_rg_bam = TMP_DIR + "/{sample}_sort_uniq_dedup_rg.bam"
    params:
        picard_dir = PICARD_DIR,
        sambamba_dir = SAMBAMBA_DIR,
        tmp_dir = TMP_DIR,
        threads = 1
    shell:
        """
        java -jar {params.picard_dir} AddOrReplaceReadGroups \
        I={input.dedup_bam} O={output.dedup_rg_bam} SO=coordinate \
        RGLB={wildcards.sample} RGPL=illumina RGPU=longevity RGSM={wildcards.sample} && \
        {params.sambamba_dir} index -t {params.threads} {output.dedup_rg_bam}
        """

rule base_recalibration:
    conda:
        "crvi_env.yml"
    input:
        dedup_rg_bam = TMP_DIR + "/{sample}_sort_uniq_dedup_rg.bam"
    output:
        recal_bam = TMP_DIR + "/{sample}_recal.bam"
    params:
        ref_dir = REF_DIR,
        gatk_dir = GATK_DIR,
        variants_dir = VARIANTS_DIR,
        tmp_dir = TMP_DIR,
        sambamba_dir = SAMBAMBA_DIR,
        threads = 1
    shell:
        """
        {params.gatk_dir} BaseRecalibrator -R {params.ref_dir} -I {input.dedup_rg_bam} \
        -output {params.tmp_dir}/{wildcards.sample}.recal.table -known-sites {params.variants_dir} && \
        {params.gatk_dir} ApplyBQSR -R {params.ref_dir} -I {input.dedup_rg_bam} \
        --bqsr-recal-file {params.tmp_dir}/{wildcards.sample}.recal.table -O {output.recal_bam} && \
        {params.sambamba_dir} index -t {params.threads} {output.recal_bam}
        """

rule haplotype_caller:
    conda:
        "crvi_env.yml"
    input:
        recal_bam = TMP_DIR + "/{sample}_recal.bam"
    output:
        recal_vcf = TMP_DIR + "/{sample}.recal.vcf.gz"
    params:
        ref_dir = REF_DIR,
        gatk_dir = GATK_DIR,
        tmp_dir = TMP_DIR,
        threads = 1
    shell:
        """
        export OMP_NUM_THREADS={params.threads} && \
        {params.gatk_dir} HaplotypeCaller -R {params.ref_dir} -I {input.recal_bam} -ERC GVCF \
        -output {params.tmp_dir}/{wildcards.sample}.recal.vcf -max-alternate-alleles 2 --native-pair-hmm-threads {params.threads} && \
        bgzip {params.tmp_dir}/{wildcards.sample}.recal.vcf && \
        tabix -p vcf {output.recal_vcf}
        """

rule combine_gvcfs:
    conda:
        "crvi_env.yml"
    input:
        gvcf_files = expand(TMP_DIR + "/{sample}.recal.vcf.gz", sample=SAMPLES),
    output:
        database_dir = directory(TMP_DIR + "/joint_calls/genomicsdb_{interval}")
    params:
        ref_dir = REF_DIR,
        gatk_dir = GATK_DIR,
        tmp_dir = TMP_DIR,
        threads = 1
    shell:
        """
        mkdir -p {params.tmp_dir}/joint_calls/ && \
        {params.gatk_dir} --java-options "-Xmx24g" GenomicsDBImport \
        --genomicsdb-workspace-path {output.database_dir} \
        --genomicsdb-shared-posixfs-optimizations true \
        --reader-threads {params.threads} \
        -R {params.ref_dir} \
        --batch-size 50 \
        --intervals {wildcards.interval} \
        $(for gvcf in {input.gvcf_files}; do echo "-V ${{gvcf}}"; done)
        """

rule genotype_gvcfs:
    conda:
        "crvi_env.yml"
    input:
        database_dir = rules.combine_gvcfs.output.database_dir
    output:
        snps_filt_vcf_gz = TMP_DIR + "/joint_call_{interval}_longevity.SNPs.vcf",
    params:
        gatk_dir = GATK_DIR,
        ref_dir = REF_DIR,
        out_dir = TMP_DIR,
        threads = 1,
        java_mem = "24g"
    shell:
        """
        # Define temporary file names in the shell command
        snps_vcf="{params.out_dir}/joint_call_{wildcards.interval}_longevity.SNPs.vcf"

        {params.gatk_dir} --java-options "-Xmx{params.java_mem} -XX:+UseParallelGC" GenotypeGVCFs \
            -R {params.ref_dir} -V gendb://{input.database_dir} -O $snps_vcf \
            --tmp-dir {params.out_dir} \
            --use-new-qual-calculator true\
            --genomicsdb-shared-posixfs-optimizations true
        """

rule select_snps:
    input:
        vcf= rules.genotype_gvcfs.output.snps_filt_vcf_gz
    output:
        selected_vcf= TMP_DIR + "/{interval}_selected.vcf"
    params:
        gatk_dir = GATK_DIR,
        ref_dir = REF_DIR,
        java_mem = "24g"
    shell:
        """
        {params.gatk_dir} --java-options "-Xmx{params.java_mem}" SelectVariants \
        -R {params.ref_dir} \
        -V {input.vcf} \
        --select-type-to-include SNP \
        -O {output.selected_vcf}
        """

rule filter_snps:
    input:
        vcf= rules.select_snps.output.selected_vcf
    output:
        filtered_vcf= TMP_DIR + "/{interval}_filtered.vcf"
    params:
        gatk_dir = GATK_DIR,
        ref_dir = REF_DIR,
        java_mem = "24g"
    shell:
        """
        {params.gatk_dir} --java-options "-Xmx{params.java_mem}" VariantFiltration \
        -R {params.ref_dir} \
        -V {input.vcf} \
        --filter-expression "QUAL < 20.0" --filter-name "LowQual" \
        --filter-expression "QD < 2.0" --filter-name "LowQD" \
        --filter-expression "FS > 60.0" --filter-name "HighFS" \
        --filter-expression "MQ < 35.0" --filter-name "LowMQ" \
        --filter-expression "MQRankSum < -12.5" --filter-name "LowMQRankSum" \
        --filter-expression "ReadPosRankSum < -8.0" --filter-name "LowReadPosRankSum" \
        -O {output.filtered_vcf}
        """

rule exclude_filtered_snps:
    input:
        vcf= rules.filter_snps.output.filtered_vcf
    output:
        filtered_excluded_vcf= TMP_DIR + "/{interval}_filtered_excluded.vcf"
    params:
        gatk_dir = GATK_DIR,
        ref_dir = REF_DIR,
        java_mem = "24g"
    shell:
        """
        {params.gatk_dir} --java-options "-Xmx{params.java_mem}" SelectVariants \
        -R {params.ref_dir} \
        -V {input.vcf} \
        --exclude-filtered \
        -O {output.filtered_excluded_vcf}
        """

def get_intervals_for_chromosome(chrom):
    return [interval for interval in INTERVALS if interval.startswith(chrom)]

rule combine_vcfs:
    input:
        lambda wildcard: [TMP_DIR + "/{}_filtered_excluded.vcf".format(interval) for interval in INTERVALS if interval.startswith(wildcard.chrom)]
    output:
        combined_vcf= RESULTS_DIR + "/{chrom}.raw.combined.vcf.gz"
    params:
        gatk_dir = GATK_DIR,
        java_mem = "196g",
        results_dir = RESULTS_DIR
    shell:
        """
        mkdir -p {params.results_dir} && \
        {params.gatk_dir} --java-options "-Xmx{params.java_mem}" GatherVcfs \
        $(for vcf in {input}; do echo "-I ${{vcf}}"; done) \
        -O {output.combined_vcf}
        """

rule filter_snps_after_merge:
    conda:
        "crvi_env.yml"
    input:
        vcf = rules.combine_vcfs.output.combined_vcf
    output:
        regions_vcf = RESULTS_DIR + "/{chrom}.filtered.combined.vcf.gz"
    params:
        out_DIR = RESULTS_DIR
    shell:
        r"""
        tabix -p vcf -f {input.vcf} && \
        plink --vcf {input.vcf} --double-id --allow-extra-chr --maf 0.05 --set-missing-var-ids '@:#:$1,$2' --indep-pairwise 200 25 0.8 --geno 0.75 --biallelic-only strict --recode vcf --out {params.out_DIR}/joint_call_{wildcards.chrom}.SNPs.PASS && \
        sed -e 's/:\t/\t/g' {params.out_DIR}/joint_call_{wildcards.chrom}.SNPs.PASS.prune.in | awk -F: '{{OFS="\t";print $1,$2-1,$2}}' > {params.out_DIR}/joint_call_{wildcards.chrom}.SNPs.PASS.bed && \
        bgzip {params.out_DIR}/joint_call_{wildcards.chrom}.SNPs.PASS.vcf && \
        tabix -p vcf {params.out_DIR}/joint_call_{wildcards.chrom}.SNPs.PASS.vcf.gz && \
        bcftools filter -e "QUAL < 20" --threads 4 -R {params.out_DIR}/joint_call_{wildcards.chrom}.SNPs.PASS.bed -O z -o {output.regions_vcf} {input.vcf} && \
        tabix -p vcf {output.regions_vcf}
        """


rule extract_counts:
    input:
        vcf_file = rules.combine_vcfs.output.combined_vcf,
        regions = rules.filter_snps_after_merge.output.regions_vcf
    output:
        out_file1 = TMP_DIR + "/count_info/joint_call_{chrom}.DP.filt.txt",
        out_file2 = TMP_DIR + "/count_info/joint_call_{chrom}.AD.filt.txt"
    shell:
        r"""
        module load bcftools && \
        mkdir -p {TMP_DIR}/count_info && \
        bcftools query -f '[%CHROM:%POS\t%SAMPLE\t%DP\n]' --regions-file {input.regions} {input.vcf_file} | sed 's/,/\t/g' > {TMP_DIR}/count_info/joint_call_{wildcards.chrom}.DP.txt && \
        bcftools query -f '[%CHROM:%POS\t%SAMPLE\t%AD\n]' --regions-file {input.regions} {input.vcf_file} | sed 's/,/\t/g' > {TMP_DIR}/count_info/joint_call_{wildcards.chrom}.AD.txt && \
        awk '{{OFS="\t"; if ($3>0) print $1,$2,$3}}' {TMP_DIR}/count_info/joint_call_{wildcards.chrom}.DP.txt > {output.out_file1} && \
        awk '{{OFS="\t"; if ($3>0 || $4>0) print $1,$2,$3,$4}}' {TMP_DIR}/count_info/joint_call_{wildcards.chrom}.AD.txt > {output.out_file2}
        """

rule process_counts:
    input:
        ad_file = TMP_DIR + "/count_info/joint_call_{chrom}.AD.filt.txt",
        dp_file = TMP_DIR + "/count_info/joint_call_{chrom}.DP.filt.txt"
    output:
        alt_counts = TMP_DIR + "/count_info/alternate_counts_{chrom}.txt",
        ref_counts = TMP_DIR + "/count_info/reference_counts_{chrom}.txt"
    shell:
        """
        module load R && \
        Rscript --vanilla /Genomics/ayroleslab2/scott/git/chromium/sequence_processing/snakemake/process_counts.R {input.ad_file} {input.dp_file} {output.alt_counts} {output.ref_counts}
        """
